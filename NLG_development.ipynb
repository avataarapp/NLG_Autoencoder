{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing this paper: [Unsupervised Natural Language Generation with Denoising Autoencoders](https://arxiv.org/pdf/1804.07899.pdf)\n",
    "\n",
    "Data from here: http://www.macs.hw.ac.uk/InteractionLab/E2E/#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For E2E Dataset\n",
    "trainset = pd.read_csv('e2e-dataset/trainset.csv')\n",
    "trainset = trainset.assign(clean=utils.replace_punctuation(trainset['ref']))\n",
    "vocab_to_int, int_to_vocab = utils.get_tokens(trainset['clean'])\n",
    "as_tokens = trainset['clean'].apply(lambda x: [vocab_to_int[each] for each in x.split()])\n",
    "trainset = trainset.assign(tokenized=as_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(dataset, p_drop=0.6, max_length=50):\n",
    "    \n",
    "    # Corrupt dataset by randomly dropping words\n",
    "    corrupted = utils.corrupt(dataset)\n",
    "    # Shuffle words in each sequence\n",
    "    shuffled = [utils.shuffle(seq, cor_seq) for seq, cor_seq in zip(dataset, corrupted)]\n",
    "\n",
    "    for shuffled_seq, original_seq in zip(shuffled, dataset):\n",
    "        # need to make sure our input_tensors have at least one element\n",
    "        if len(shuffled_seq) == 0:\n",
    "            shuffled_seq = [original_seq[np.random.randint(0, len(original_seq))]]\n",
    "        \n",
    "        input_tensor = torch.Tensor(shuffled_seq).view(-1, 1).type(torch.LongTensor)\n",
    "        \n",
    "        # Append <EOS> token to the end of original sequence\n",
    "        target = original_seq.copy()\n",
    "        target.append(1)\n",
    "        target_tensor = torch.Tensor(target).view(-1, 1).type(torch.LongTensor)\n",
    "            \n",
    "        yield input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size=300, hidden_size=256, num_layers=2, drop_p=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, \n",
    "                            dropout=drop_p, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, device='cpu'):\n",
    "        \"\"\" Create two tensors with shape (num_layers * num_directions, batch, hidden_size)\n",
    "            for the hidden state and cell state\n",
    "        \"\"\"\n",
    "        h_0, c_0 = torch.zeros(2, 2*self.num_layers, 1, self.hidden_size, device=device)\n",
    "        \n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention network from http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size=300, hidden_size=256, \n",
    "                       num_layers=2, drop_p=0.1, max_length=50):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.attn = nn.Linear(self.hidden_size + embedding_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2 + embedding_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, \n",
    "                            dropout=drop_p, bidirectional=True)\n",
    "        \n",
    "        self.out = nn.Linear(2 * hidden_size, vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Learns the attention vector (a probability distribution) here for weighting\n",
    "        # encoder outputs based on the decoder input and encoder hidden vector\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1)\n",
    "        \n",
    "        # Applies the attention vector (again, a probability distribution) to the encoder\n",
    "        # outputs which weight the encoder_outputs\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        # Now the decoder input is combined with the weighted encoder_outputs and\n",
    "        # passed through a linear transformation as input to the LSTM layer\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output).view(1, -1)\n",
    "        output = self.log_softmax(output)\n",
    "    \n",
    "        return output, hidden, attn_weights\n",
    "        \n",
    "    def init_hidden(self, device='cpu'):\n",
    "        \"\"\" Create two tensors with shape (num_layers * num_directions, batch, hidden_size)\n",
    "            for the hidden state and cell state\n",
    "        \"\"\"\n",
    "        h_0, c_0 = torch.zeros(2, 2*self.num_layers, 1, self.hidden_size, device=device)\n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, encoder, decoder, enc_opt, dec_opt, criterion, \n",
    "          max_length=50, print_every=1000, plot_every=100, \n",
    "          teacher_forcing=0.5, device=None):\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    steps = 0\n",
    "    plot_losses = []\n",
    "    for input_tensor, target_tensor in dataloader(dataset):\n",
    "        loss = 0\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        input_tensor = input_tensor.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "\n",
    "        enc_opt.zero_grad()\n",
    "        dec_opt.zero_grad()\n",
    "\n",
    "        h, c = encoder.init_hidden(device=device)\n",
    "        encoder_outputs = torch.zeros(max_length, 2*encoder.hidden_size).to(device)\n",
    "\n",
    "        # Run input through encoder\n",
    "        enc_outputs, enc_hidden = encoder.forward(input_tensor, (h, c))\n",
    "        \n",
    "        # Prepare encoder_outputs for attention\n",
    "        encoder_outputs[:min(enc_outputs.shape[0], max_length)] = enc_outputs[:max_length,0,:]\n",
    "\n",
    "        # First decoder input is the <SOS> token\n",
    "        dec_input = torch.Tensor([[0]]).type(torch.LongTensor).to(device)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_outputs = []\n",
    "        for ii in range(target_tensor.shape[0]):\n",
    "            # Pass in previous output and hidden state\n",
    "            dec_out, dec_hidden, dec_attn = decoder.forward(dec_input, dec_hidden, encoder_outputs)\n",
    "            _, out_token = dec_out.topk(1)\n",
    "            \n",
    "            # Curriculum learning, sometimes use the decoder output as the next input,\n",
    "            # sometimes use the correct token from the target sequence\n",
    "            if np.random.rand() < teacher_forcing:\n",
    "                dec_input = target_tensor[ii].view(*out_token.shape)\n",
    "            else:\n",
    "                dec_input = out_token.detach().to(device)  # detach from history as input\n",
    "            \n",
    "            dec_outputs.append(out_token)\n",
    "\n",
    "            loss += criterion(dec_out, target_tensor[ii])\n",
    "            \n",
    "            # If the input is the <EOS> token (end of sentence)...\n",
    "            if dec_input.item() == 1:\n",
    "                break\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(encoder.parameters(), 5)\n",
    "        nn.utils.clip_grad_norm_(decoder.parameters(), 5)\n",
    "\n",
    "        enc_opt.step()\n",
    "        dec_opt.step()\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if steps % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f\"Loss avg. = {print_loss_avg}\")\n",
    "            print([int_to_vocab[each.item()] for each in input_tensor])\n",
    "            print([int_to_vocab[each.item()] for each in dec_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# max length for attention\n",
    "max_length = 50\n",
    "\n",
    "encoder = Encoder(len(vocab_to_int), hidden_size=512, drop_p=0.1).to(device)\n",
    "decoder = Decoder(len(vocab_to_int), hidden_size=512, drop_p=0.1, max_length=max_length).to(device)\n",
    "\n",
    "enc_opt = optim.Adam(encoder.parameters(), lr=0.001, amsgrad=True)\n",
    "dec_opt = optim.Adam(decoder.parameters(), lr=0.001, amsgrad=True)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss avg. = 0.007070066407322884\n",
      "['coffee', 'city', 'centre', 'is', 'a', 'an']\n",
      "['The', 'Rice', 'Curry', 'is', 'a', 'family', 'restaurant', 'restaurant', 'shop', 'located', 'near', 'the', 'riverside', 'centre', 'near', 'near', 'has', 'a', 'average', 'customer', 'rating', '<PERIOD>', '<EOS>']\n",
      "Loss avg. = 0.03228044509887695\n",
      "['<COMMA>', 'The', 'customer', 'Rating', 'that', 'out', 'a', 'located', 'near', 'Plaza', 'Browns', 'Cambridge', 'is', 'is', 'of', '<PERIOD>', 'The']\n",
      "['The', 'Cambridge', 'is', 'a', 'coffee', 'shop', '<COMMA>', 'in', 'the', '<COMMA>', 'near', '<COMMA>', 'Hotel', '<COMMA>', 'It', 'price', 'is', 'is', 'not', 'for', 'children', 'food', 'It', 'price', 'is', 'not', 'friendly', '<PERIOD>', 'has', 'Rice', 'rating', 'is', 'low', 'low', 'out', 'of', 'of', '5', '<PERIOD>', '<EOS>']\n",
      "Loss avg. = 0.007794644217938185\n",
      "['in', 'one', 'star', 'Curry', 'is', 'a', 'family', 'near', 'the']\n",
      "['The', 'Waterman', 'Curry', 'is', 'a', 'family', 'friendly', 'restaurant', 'star', 'restaurant', 'located', 'located', 'the', 'river', 'Rouge', '<PERIOD>', 'the', 'city', 'area', '<PERIOD>', '<EOS>']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e58e76b40466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     train(trainset['tokenized'], encoder, decoder, enc_opt, dec_opt, criterion, \n\u001b[1;32m      5\u001b[0m           \u001b[0mteacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           max_length=max_length)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-f67c6823d296>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, encoder, decoder, enc_opt, dec_opt, criterion, max_length, print_every, plot_every, teacher_forcing, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Pass in previous output and hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mdec_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b65efdbfe476>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             dropout_ts)\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/mat/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m(287)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    285 \u001b[0;31m            \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    286 \u001b[0;31m            \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 287 \u001b[0;31m            dropout_ts)\n",
      "\u001b[0m\u001b[0;32m    288 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    289 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for e in range(1, epochs+1):\n",
    "    print(f\"Starting epoch {e}\")\n",
    "    train(trainset['tokenized'], encoder, decoder, enc_opt, dec_opt, criterion, \n",
    "          teacher_forcing=0.9/e, device=device, print_every=4200,\n",
    "          max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\"hidden_size\": 256,\n",
    "              \"num_layers\": 512,\n",
    "              \"encoder_sd\": encoder.state_dict(),\n",
    "              \"decoder_sd\": decoder.state_dict(),\n",
    "              \"epochs\": 5}\n",
    "\n",
    "torch.save(checkpoint, \"nlg_07052018.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
