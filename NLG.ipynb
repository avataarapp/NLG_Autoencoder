{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.read_csv('e2e-dataset/trainset.csv')\n",
    "trainset = trainset.assign(clean=utils.replace_punctuation(trainset['ref']))\n",
    "vocab_to_int, int_to_vocab = utils.get_tokens(trainset['clean'])\n",
    "as_tokens = trainset['clean'].apply(lambda x: [vocab_to_int[each] for each in x.split()])\n",
    "trainset = trainset.assign(tokenized=as_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(dataset, p_drop=0.6, max_length=50):\n",
    "    \n",
    "    # Corrupt dataset by randomly dropping words\n",
    "    corrupted = utils.corrupt(dataset)\n",
    "    # Shuffle words in each sequence\n",
    "    shuffled = [utils.shuffle(seq, cor_seq) for seq, cor_seq in zip(dataset, corrupted)]\n",
    "\n",
    "    for shuffled_seq, original_seq in zip(shuffled, dataset):\n",
    "        # need to make sure our input_tensors have at least one element\n",
    "        if len(shuffled_seq) == 0:\n",
    "            shuffled_seq = [original_seq[np.random.randint(0, len(original_seq))]]\n",
    "        \n",
    "        input_tensor = torch.Tensor(shuffled_seq).view(-1, 1).type(torch.LongTensor)\n",
    "        \n",
    "        # Append <EOS> token to the end of original sequence\n",
    "        target = original_seq.copy()\n",
    "        target.append(1)\n",
    "        target_tensor = torch.Tensor(target).view(-1, 1).type(torch.LongTensor)\n",
    "            \n",
    "        yield input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size=300, hidden_size=256, num_layers=2, drop_p=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, \n",
    "                            dropout=drop_p, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, device='cpu'):\n",
    "        \"\"\" Create two tensors with shape (num_layers * num_directions, batch, hidden_size)\n",
    "            for the hidden state and cell state\n",
    "        \"\"\"\n",
    "        h_0, c_0 = torch.zeros(2, 2*self.num_layers, 1, self.hidden_size, device=device)\n",
    "        \n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention network from http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size=300, hidden_size=256, \n",
    "                       num_layers=2, drop_p=0.1, max_length=50):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.attn = nn.Linear(self.hidden_size + embedding_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2 + embedding_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, \n",
    "                            dropout=drop_p, bidirectional=True)\n",
    "        \n",
    "        self.out = nn.Linear(2 * hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Learns the attention vector (a probability distribution) here for weighting\n",
    "        # encoder outputs based on the decoder input and encoder hidden vector\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1)\n",
    "        \n",
    "        # Applies the attention vector (again, a probability distribution) to the encoder\n",
    "        # outputs which weight the encoder_outputs\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        # Now the decoder input is combined with the weighted encoder_outputs and\n",
    "        # passed through a linear transformation as input to the LSTM layer\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output).view(1, -1)\n",
    "        output = self.softmax(output)\n",
    "    \n",
    "        return output, hidden, attn_weights\n",
    "        \n",
    "    def init_hidden(self, device='cpu'):\n",
    "        \"\"\" Create two tensors with shape (num_layers * num_directions, batch, hidden_size)\n",
    "            for the hidden state and cell state\n",
    "        \"\"\"\n",
    "        h_0, c_0 = torch.zeros(2, 2*self.num_layers, 1, self.hidden_size, device=device)\n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, encoder, decoder, enc_opt, dec_opt, criterion, \n",
    "          max_length=50, print_every=1000, plot_every=100, \n",
    "          teacher_forcing=0.5, device=None):\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    steps = 0\n",
    "    plot_losses = []\n",
    "    for input_tensor, target_tensor in dataloader(dataset):\n",
    "        loss = 0\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        input_tensor = input_tensor.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "\n",
    "        enc_opt.zero_grad()\n",
    "        dec_opt.zero_grad()\n",
    "\n",
    "        h, c = encoder.init_hidden(device=device)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, 2*encoder.hidden_size).to(device)\n",
    "\n",
    "        # Run input through encoder\n",
    "        enc_outputs, enc_hidden = encoder.forward(input_tensor, (h, c))\n",
    "        \n",
    "        # Prepare encoder_outputs for attention \n",
    "        encoder_outputs[:enc_outputs.shape[0]] = enc_outputs.squeeze()\n",
    "\n",
    "        # First decoder input is the <SOS> token\n",
    "        dec_input = torch.Tensor([[0]]).type(torch.LongTensor).to(device)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_outputs = []\n",
    "        for ii in range(target_tensor.shape[0]):\n",
    "            # Pass in previous output and hidden state\n",
    "            dec_out, dec_hidden, dec_attn = decoder.forward(dec_input, dec_hidden, encoder_outputs)\n",
    "            _, out_token = dec_out.topk(1)\n",
    "            \n",
    "            # Curriculum learning, sometimes use the decoder output as the next input,\n",
    "            # sometimes use the correct token from the target sequence\n",
    "            if np.random.rand() < teacher_forcing:\n",
    "                dec_input = target_tensor[ii].view(*out_token.shape)\n",
    "            else:\n",
    "                dec_input = out_token.detach().to(device)  # detach from history as input\n",
    "            \n",
    "            dec_outputs.append(out_token)\n",
    "\n",
    "            loss += criterion(dec_out, target_tensor[ii])\n",
    "            \n",
    "            # If the input is the <EOS> token (end of sentence)...\n",
    "            if dec_input.item() == 1:\n",
    "                break\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(encoder.parameters(), 5)\n",
    "        nn.utils.clip_grad_norm_(decoder.parameters(), 5)\n",
    "\n",
    "        enc_opt.step()\n",
    "        dec_opt.step()\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if steps % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f\"Loss avg. = {print_loss_avg}\")\n",
    "            print([int_to_vocab[each.item()] for each in input_tensor])\n",
    "            print([int_to_vocab[each.item()] for each in dec_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# max length for attention\n",
    "max_length = 50\n",
    "\n",
    "encoder = Encoder(len(vocab_to_int), drop_p=0).to(device)\n",
    "decoder = Decoder(len(vocab_to_int), drop_p=0, max_length=max_length).to(device)\n",
    "\n",
    "enc_opt = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "dec_opt = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss avg. = 0.024222789332270622\n",
      "['be', 'found', 'near', '<COMMA>', 'and', 'Sicilia', 'price', '<COMMA>']\n",
      "['The', 'is', 'is', 'a', 'food', 'in', 'range', '<COMMA>', 'a', '<COMMA>', 'the', 'city', 'centre', '<PERIOD>', 'is', 'Indian', 'food', 'is', 'is', 'not', 'in', 'the', 'the', 'riverside', '<PERIOD>', '<EOS>']\n",
      "Loss avg. = 0.02245480939745903\n",
      "['A', 'kid', 'open', '<COMMA>', 'moderately', 'now', 'restaurant', '<COMMA>', 'French']\n",
      "['Near', 'the', 'Crowne', 'Plaza', 'Hotel', '<COMMA>', '<COMMA>', 'food', '<COMMA>', 'The', 'The', 'Waterman', '<COMMA>', 'a', '<PERIOD>', '<EOS>']\n",
      "Loss avg. = 0.012393290176987648\n",
      "['<COMMA>', 'near', 'Golden', 'English', 'food', 'by', 'the', '1', '<PERIOD>', 'It', 'is']\n",
      "['The', 'Golden', 'Curry', 'is', 'a', 'food', '<COMMA>', 'the', 'riverside', '<PERIOD>', 'It', 'Café', 'Rouge', '<COMMA>', 'It', 'is', 'a', 'family', '<PERIOD>', 'has', 'a', 'high', 'out', 'of', '5', '<PERIOD>', '<EOS>']\n",
      "Loss avg. = 0.014129290357232094\n",
      "['high', 'priced', 'not', 'kid', 'center', 'foods', 'in', 'the']\n",
      "['The', 'Waterman', 'is', 'pub', 'friendly', 'pub', 'restaurant', 'priced', 'restaurant', 'kid', 'friendly', 'pub', 'English', 'food', 'in', 'the', 'riverside', 'area', '<PERIOD>', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for e in range(1, epochs+1):\n",
    "    print(f\"Starting epoch {e}\")\n",
    "    train(trainset['tokenized'], encoder, decoder, enc_opt, dec_opt, criterion, \n",
    "          teacher_forcing=0.5/e, device=device, print_every=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
